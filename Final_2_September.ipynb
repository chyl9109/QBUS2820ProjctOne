{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from QBUS2820 import rmse_jack, r2_jack \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from QBUS2820 import forward\n",
    "from sklearn.ensemble import ExtraTreesRegressor, BaggingRegressor, GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor,NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using standardised data for LASSO, and Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('TrainStandard.csv')\n",
    "final_train = data.sample(frac=0.6, random_state=1)\n",
    "final_test = data[data.index.isin(final_train.index)==False]\n",
    "final_train.head()\n",
    "y_train = final_train.pop('SalePrice')\n",
    "y_test = final_test.pop('SalePrice')\n",
    "\n",
    "y_train\n",
    "mu=y_train.mean()\n",
    "sigma=y_train.std() \n",
    "\n",
    "standardPrice=(y_train-mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = []\n",
    "pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LASSO\n",
    "lasso = LassoCV(cv=10)\n",
    "lasso.fit(final_train, np.ravel(standardPrice)) \n",
    "pred_L = lasso.predict(final_test)\n",
    "predFinalLasso = (pred_L*sigma) + mu\n",
    "method.append('LASSO')\n",
    "pred.append(predFinalLasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RIDGE\n",
    "alphas = np.exp(np.linspace(-10,20,500)) \n",
    "ridge = RidgeCV(alphas=alphas, cv=10)\n",
    "ridge.fit(final_train, np.ravel(standardPrice))\n",
    "ridge = Ridge(alpha=ridge.alpha_)\n",
    "ridge.fit(final_train, np.ravel(standardPrice))\n",
    "pred_R = ridge.predict(final_test)\n",
    "predFinalRidge = (pred_R*sigma) + mu\n",
    "method.append('Ridge')\n",
    "pred.append(predFinalRidge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENET\n",
    "#Elastic Net\n",
    "\n",
    "enet = ElasticNetCV(l1_ratio=[0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.99], cv=10)\n",
    "enet.fit(final_train, np.ravel(standardPrice))\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "enet = ElasticNet(alpha=enet.alpha_, l1_ratio=enet.l1_ratio_)\n",
    "enet.fit(final_train, np.ravel(standardPrice))\n",
    "pred_E = enet.predict(final_test)\n",
    "predFinalEnet = (pred_E*sigma) + mu\n",
    "method.append('ENET')\n",
    "pred.append(predFinalEnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using a regular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('TrainSale1.csv')\n",
    "final_train = data.sample(frac=0.6, random_state=1)\n",
    "final_test = data[data.index.isin(final_train.index)==False]\n",
    "final_train.head()\n",
    "y_train = final_train.pop('SalePrice')\n",
    "y_test = final_test.pop('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Forward with non-standardised data\n",
    "fwd = forward()\n",
    "fwd.fit(final_train, y_train)\n",
    "predFinalForwardNormal = fwd.predict(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "method.append('Forward Sel')\n",
    "pred.append(predFinalForwardNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using esemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We specify a loss function of either least square (LS) or least absolute deviation (LAD)\n",
    "### LAD does better if outliers present and LS does better if no outliers present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>SE</th>\n",
       "      <th>Jack R2</th>\n",
       "      <th>SE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R-square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.0001 rate and square type</th>\n",
       "      <td>29649.544</td>\n",
       "      <td>3433.706</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.025</td>\n",
       "      <td>21269.327</td>\n",
       "      <td>0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.0001 rate and linear type</th>\n",
       "      <td>29852.698</td>\n",
       "      <td>3033.374</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.021</td>\n",
       "      <td>21825.917</td>\n",
       "      <td>0.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.001 rate and square type</th>\n",
       "      <td>28800.346</td>\n",
       "      <td>3282.502</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20752.061</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.001 rate and linear type</th>\n",
       "      <td>29583.824</td>\n",
       "      <td>2880.191</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22100.676</td>\n",
       "      <td>0.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.01 rate and square type</th>\n",
       "      <td>28863.377</td>\n",
       "      <td>3103.781</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.021</td>\n",
       "      <td>21160.105</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.01 rate and linear type</th>\n",
       "      <td>28350.894</td>\n",
       "      <td>3007.248</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.020</td>\n",
       "      <td>20966.847</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.1 rate and square type</th>\n",
       "      <td>27659.435</td>\n",
       "      <td>2662.983</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.018</td>\n",
       "      <td>20637.702</td>\n",
       "      <td>0.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.1 rate and linear type</th>\n",
       "      <td>27369.429</td>\n",
       "      <td>2473.351</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.017</td>\n",
       "      <td>20326.353</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.2 rate and square type</th>\n",
       "      <td>26621.393</td>\n",
       "      <td>1778.866</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.015</td>\n",
       "      <td>20364.046</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.2 rate and linear type</th>\n",
       "      <td>28100.092</td>\n",
       "      <td>3912.520</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.029</td>\n",
       "      <td>20082.683</td>\n",
       "      <td>0.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.3 rate and square type</th>\n",
       "      <td>25885.195</td>\n",
       "      <td>2272.038</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.015</td>\n",
       "      <td>19224.516</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 0.3 rate and linear type</th>\n",
       "      <td>26557.626</td>\n",
       "      <td>2704.340</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.018</td>\n",
       "      <td>19176.790</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 1 rate and square type</th>\n",
       "      <td>26543.735</td>\n",
       "      <td>2772.479</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.018</td>\n",
       "      <td>19444.505</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50 estimators, 1 rate and linear type</th>\n",
       "      <td>26141.761</td>\n",
       "      <td>3117.830</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.021</td>\n",
       "      <td>18505.878</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.0001 rate and square type</th>\n",
       "      <td>28614.813</td>\n",
       "      <td>2707.012</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.018</td>\n",
       "      <td>21258.344</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.0001 rate and linear type</th>\n",
       "      <td>29104.769</td>\n",
       "      <td>2973.439</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.021</td>\n",
       "      <td>21392.336</td>\n",
       "      <td>0.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.001 rate and square type</th>\n",
       "      <td>29270.907</td>\n",
       "      <td>3199.907</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.023</td>\n",
       "      <td>21380.180</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.001 rate and linear type</th>\n",
       "      <td>29162.558</td>\n",
       "      <td>3511.738</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.025</td>\n",
       "      <td>21281.615</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.01 rate and square type</th>\n",
       "      <td>28610.634</td>\n",
       "      <td>3140.699</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.021</td>\n",
       "      <td>20999.709</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.01 rate and linear type</th>\n",
       "      <td>28763.658</td>\n",
       "      <td>3058.274</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.021</td>\n",
       "      <td>21095.049</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.1 rate and square type</th>\n",
       "      <td>27423.011</td>\n",
       "      <td>3045.964</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.021</td>\n",
       "      <td>20031.869</td>\n",
       "      <td>0.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.1 rate and linear type</th>\n",
       "      <td>25951.938</td>\n",
       "      <td>2236.534</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.015</td>\n",
       "      <td>19524.043</td>\n",
       "      <td>0.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.2 rate and square type</th>\n",
       "      <td>25670.249</td>\n",
       "      <td>2421.234</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18818.069</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.2 rate and linear type</th>\n",
       "      <td>26383.095</td>\n",
       "      <td>2859.417</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.019</td>\n",
       "      <td>19183.674</td>\n",
       "      <td>0.858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.3 rate and square type</th>\n",
       "      <td>26442.070</td>\n",
       "      <td>2912.909</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.020</td>\n",
       "      <td>19104.798</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 0.3 rate and linear type</th>\n",
       "      <td>25594.677</td>\n",
       "      <td>2572.790</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.016</td>\n",
       "      <td>18567.308</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 1 rate and square type</th>\n",
       "      <td>24707.383</td>\n",
       "      <td>2385.636</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18161.889</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100 estimators, 1 rate and linear type</th>\n",
       "      <td>25212.416</td>\n",
       "      <td>2241.918</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18728.587</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150 estimators, 0.0001 rate and square type</th>\n",
       "      <td>29210.729</td>\n",
       "      <td>3208.330</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.022</td>\n",
       "      <td>21435.675</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150 estimators, 0.0001 rate and linear type</th>\n",
       "      <td>29175.973</td>\n",
       "      <td>3480.511</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.025</td>\n",
       "      <td>21298.841</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250 estimators, 1 rate and square type</th>\n",
       "      <td>24562.191</td>\n",
       "      <td>2345.910</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18199.074</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250 estimators, 1 rate and linear type</th>\n",
       "      <td>24950.459</td>\n",
       "      <td>2222.688</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18286.245</td>\n",
       "      <td>0.873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.0001 rate and square type</th>\n",
       "      <td>28869.840</td>\n",
       "      <td>3074.738</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.021</td>\n",
       "      <td>21152.016</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.0001 rate and linear type</th>\n",
       "      <td>28866.715</td>\n",
       "      <td>3006.378</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.021</td>\n",
       "      <td>21262.754</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.001 rate and square type</th>\n",
       "      <td>28894.181</td>\n",
       "      <td>3345.924</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.023</td>\n",
       "      <td>21153.205</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.001 rate and linear type</th>\n",
       "      <td>28642.207</td>\n",
       "      <td>3355.087</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.023</td>\n",
       "      <td>20856.190</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.01 rate and square type</th>\n",
       "      <td>27527.476</td>\n",
       "      <td>2891.252</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.019</td>\n",
       "      <td>20410.665</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.01 rate and linear type</th>\n",
       "      <td>27898.314</td>\n",
       "      <td>3029.845</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.021</td>\n",
       "      <td>20523.725</td>\n",
       "      <td>0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.1 rate and square type</th>\n",
       "      <td>25847.688</td>\n",
       "      <td>2637.551</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.017</td>\n",
       "      <td>19015.790</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.1 rate and linear type</th>\n",
       "      <td>25356.968</td>\n",
       "      <td>2344.673</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18767.256</td>\n",
       "      <td>0.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.2 rate and square type</th>\n",
       "      <td>25793.496</td>\n",
       "      <td>2760.567</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.018</td>\n",
       "      <td>18867.059</td>\n",
       "      <td>0.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.2 rate and linear type</th>\n",
       "      <td>25423.522</td>\n",
       "      <td>2292.448</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18782.913</td>\n",
       "      <td>0.868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.3 rate and square type</th>\n",
       "      <td>24980.881</td>\n",
       "      <td>2387.780</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18584.452</td>\n",
       "      <td>0.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 0.3 rate and linear type</th>\n",
       "      <td>25107.374</td>\n",
       "      <td>2293.353</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18550.458</td>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 1 rate and square type</th>\n",
       "      <td>24681.203</td>\n",
       "      <td>2234.143</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.014</td>\n",
       "      <td>18427.854</td>\n",
       "      <td>0.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300 estimators, 1 rate and linear type</th>\n",
       "      <td>24775.985</td>\n",
       "      <td>2512.766</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.016</td>\n",
       "      <td>18195.216</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.0001 rate and square type</th>\n",
       "      <td>29266.270</td>\n",
       "      <td>3336.282</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.023</td>\n",
       "      <td>21308.481</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.0001 rate and linear type</th>\n",
       "      <td>28893.467</td>\n",
       "      <td>3363.231</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.024</td>\n",
       "      <td>21103.812</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.001 rate and square type</th>\n",
       "      <td>28673.264</td>\n",
       "      <td>3143.033</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.022</td>\n",
       "      <td>21128.064</td>\n",
       "      <td>0.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.001 rate and linear type</th>\n",
       "      <td>28969.424</td>\n",
       "      <td>3284.479</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.023</td>\n",
       "      <td>21234.783</td>\n",
       "      <td>0.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.01 rate and square type</th>\n",
       "      <td>28168.608</td>\n",
       "      <td>3228.215</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20645.395</td>\n",
       "      <td>0.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.01 rate and linear type</th>\n",
       "      <td>27781.267</td>\n",
       "      <td>3040.635</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.021</td>\n",
       "      <td>20452.455</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.1 rate and square type</th>\n",
       "      <td>26094.048</td>\n",
       "      <td>2721.007</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.018</td>\n",
       "      <td>19037.660</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.1 rate and linear type</th>\n",
       "      <td>25490.177</td>\n",
       "      <td>2302.100</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18707.895</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.2 rate and square type</th>\n",
       "      <td>25385.769</td>\n",
       "      <td>2271.983</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18922.455</td>\n",
       "      <td>0.868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.2 rate and linear type</th>\n",
       "      <td>25091.170</td>\n",
       "      <td>2365.984</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18450.635</td>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.3 rate and square type</th>\n",
       "      <td>25183.362</td>\n",
       "      <td>2347.778</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18646.842</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 0.3 rate and linear type</th>\n",
       "      <td>25199.878</td>\n",
       "      <td>2353.683</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18607.436</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 1 rate and square type</th>\n",
       "      <td>24565.611</td>\n",
       "      <td>2452.702</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18189.847</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350 estimators, 1 rate and linear type</th>\n",
       "      <td>24814.028</td>\n",
       "      <td>2464.688</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18188.448</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Test RMSE        SE  Jack R2  \\\n",
       "50 estimators, 0.0001 rate and square type   29649.544  3433.706    0.820   \n",
       "50 estimators, 0.0001 rate and linear type   29852.698  3033.374    0.818   \n",
       "50 estimators, 0.001 rate and square type    28800.346  3282.502    0.830   \n",
       "50 estimators, 0.001 rate and linear type    29583.824  2880.191    0.821   \n",
       "50 estimators, 0.01 rate and square type     28863.377  3103.781    0.830   \n",
       "50 estimators, 0.01 rate and linear type     28350.894  3007.248    0.836   \n",
       "50 estimators, 0.1 rate and square type      27659.435  2662.983    0.844   \n",
       "50 estimators, 0.1 rate and linear type      27369.429  2473.351    0.847   \n",
       "50 estimators, 0.2 rate and square type      26621.393  1778.866    0.855   \n",
       "50 estimators, 0.2 rate and linear type      28100.092  3912.520    0.839   \n",
       "50 estimators, 0.3 rate and square type      25885.195  2272.038    0.863   \n",
       "50 estimators, 0.3 rate and linear type      26557.626  2704.340    0.856   \n",
       "50 estimators, 1 rate and square type        26543.735  2772.479    0.856   \n",
       "50 estimators, 1 rate and linear type        26141.761  3117.830    0.860   \n",
       "100 estimators, 0.0001 rate and square type  28614.813  2707.012    0.833   \n",
       "100 estimators, 0.0001 rate and linear type  29104.769  2973.439    0.827   \n",
       "100 estimators, 0.001 rate and square type   29270.907  3199.907    0.825   \n",
       "100 estimators, 0.001 rate and linear type   29162.558  3511.738    0.826   \n",
       "100 estimators, 0.01 rate and square type    28610.634  3140.699    0.833   \n",
       "100 estimators, 0.01 rate and linear type    28763.658  3058.274    0.831   \n",
       "100 estimators, 0.1 rate and square type     27423.011  3045.964    0.846   \n",
       "100 estimators, 0.1 rate and linear type     25951.938  2236.534    0.862   \n",
       "100 estimators, 0.2 rate and square type     25670.249  2421.234    0.865   \n",
       "100 estimators, 0.2 rate and linear type     26383.095  2859.417    0.858   \n",
       "100 estimators, 0.3 rate and square type     26442.070  2912.909    0.857   \n",
       "100 estimators, 0.3 rate and linear type     25594.677  2572.790    0.866   \n",
       "100 estimators, 1 rate and square type       24707.383  2385.636    0.875   \n",
       "100 estimators, 1 rate and linear type       25212.416  2241.918    0.870   \n",
       "150 estimators, 0.0001 rate and square type  29210.729  3208.330    0.826   \n",
       "150 estimators, 0.0001 rate and linear type  29175.973  3480.511    0.826   \n",
       "...                                                ...       ...      ...   \n",
       "250 estimators, 1 rate and square type       24562.191  2345.910    0.877   \n",
       "250 estimators, 1 rate and linear type       24950.459  2222.688    0.873   \n",
       "300 estimators, 0.0001 rate and square type  28869.840  3074.738    0.830   \n",
       "300 estimators, 0.0001 rate and linear type  28866.715  3006.378    0.830   \n",
       "300 estimators, 0.001 rate and square type   28894.181  3345.924    0.829   \n",
       "300 estimators, 0.001 rate and linear type   28642.207  3355.087    0.832   \n",
       "300 estimators, 0.01 rate and square type    27527.476  2891.252    0.845   \n",
       "300 estimators, 0.01 rate and linear type    27898.314  3029.845    0.841   \n",
       "300 estimators, 0.1 rate and square type     25847.688  2637.551    0.863   \n",
       "300 estimators, 0.1 rate and linear type     25356.968  2344.673    0.869   \n",
       "300 estimators, 0.2 rate and square type     25793.496  2760.567    0.864   \n",
       "300 estimators, 0.2 rate and linear type     25423.522  2292.448    0.868   \n",
       "300 estimators, 0.3 rate and square type     24980.881  2387.780    0.872   \n",
       "300 estimators, 0.3 rate and linear type     25107.374  2293.353    0.871   \n",
       "300 estimators, 1 rate and square type       24681.203  2234.143    0.876   \n",
       "300 estimators, 1 rate and linear type       24775.985  2512.766    0.875   \n",
       "350 estimators, 0.0001 rate and square type  29266.270  3336.282    0.825   \n",
       "350 estimators, 0.0001 rate and linear type  28893.467  3363.231    0.829   \n",
       "350 estimators, 0.001 rate and square type   28673.264  3143.033    0.832   \n",
       "350 estimators, 0.001 rate and linear type   28969.424  3284.479    0.828   \n",
       "350 estimators, 0.01 rate and square type    28168.608  3228.215    0.838   \n",
       "350 estimators, 0.01 rate and linear type    27781.267  3040.635    0.842   \n",
       "350 estimators, 0.1 rate and square type     26094.048  2721.007    0.861   \n",
       "350 estimators, 0.1 rate and linear type     25490.177  2302.100    0.867   \n",
       "350 estimators, 0.2 rate and square type     25385.769  2271.983    0.868   \n",
       "350 estimators, 0.2 rate and linear type     25091.170  2365.984    0.871   \n",
       "350 estimators, 0.3 rate and square type     25183.362  2347.778    0.870   \n",
       "350 estimators, 0.3 rate and linear type     25199.878  2353.683    0.870   \n",
       "350 estimators, 1 rate and square type       24565.611  2452.702    0.877   \n",
       "350 estimators, 1 rate and linear type       24814.028  2464.688    0.874   \n",
       "\n",
       "                                                SE        MAE  R-square  \n",
       "50 estimators, 0.0001 rate and square type   0.025  21269.327     0.820  \n",
       "50 estimators, 0.0001 rate and linear type   0.021  21825.917     0.818  \n",
       "50 estimators, 0.001 rate and square type    0.022  20752.061     0.830  \n",
       "50 estimators, 0.001 rate and linear type    0.020  22100.676     0.821  \n",
       "50 estimators, 0.01 rate and square type     0.021  21160.105     0.830  \n",
       "50 estimators, 0.01 rate and linear type     0.020  20966.847     0.836  \n",
       "50 estimators, 0.1 rate and square type      0.018  20637.702     0.844  \n",
       "50 estimators, 0.1 rate and linear type      0.017  20326.353     0.847  \n",
       "50 estimators, 0.2 rate and square type      0.015  20364.046     0.855  \n",
       "50 estimators, 0.2 rate and linear type      0.029  20082.683     0.839  \n",
       "50 estimators, 0.3 rate and square type      0.015  19224.516     0.863  \n",
       "50 estimators, 0.3 rate and linear type      0.018  19176.790     0.856  \n",
       "50 estimators, 1 rate and square type        0.018  19444.505     0.856  \n",
       "50 estimators, 1 rate and linear type        0.021  18505.878     0.860  \n",
       "100 estimators, 0.0001 rate and square type  0.018  21258.344     0.833  \n",
       "100 estimators, 0.0001 rate and linear type  0.021  21392.336     0.827  \n",
       "100 estimators, 0.001 rate and square type   0.023  21380.180     0.825  \n",
       "100 estimators, 0.001 rate and linear type   0.025  21281.615     0.826  \n",
       "100 estimators, 0.01 rate and square type    0.021  20999.709     0.833  \n",
       "100 estimators, 0.01 rate and linear type    0.021  21095.049     0.831  \n",
       "100 estimators, 0.1 rate and square type     0.021  20031.869     0.846  \n",
       "100 estimators, 0.1 rate and linear type     0.015  19524.043     0.862  \n",
       "100 estimators, 0.2 rate and square type     0.015  18818.069     0.865  \n",
       "100 estimators, 0.2 rate and linear type     0.019  19183.674     0.858  \n",
       "100 estimators, 0.3 rate and square type     0.020  19104.798     0.857  \n",
       "100 estimators, 0.3 rate and linear type     0.016  18567.308     0.866  \n",
       "100 estimators, 1 rate and square type       0.015  18161.889     0.875  \n",
       "100 estimators, 1 rate and linear type       0.015  18728.587     0.870  \n",
       "150 estimators, 0.0001 rate and square type  0.022  21435.675     0.826  \n",
       "150 estimators, 0.0001 rate and linear type  0.025  21298.841     0.826  \n",
       "...                                            ...        ...       ...  \n",
       "250 estimators, 1 rate and square type       0.015  18199.074     0.877  \n",
       "250 estimators, 1 rate and linear type       0.015  18286.245     0.873  \n",
       "300 estimators, 0.0001 rate and square type  0.021  21152.016     0.830  \n",
       "300 estimators, 0.0001 rate and linear type  0.021  21262.754     0.830  \n",
       "300 estimators, 0.001 rate and square type   0.023  21153.205     0.829  \n",
       "300 estimators, 0.001 rate and linear type   0.023  20856.190     0.832  \n",
       "300 estimators, 0.01 rate and square type    0.019  20410.665     0.845  \n",
       "300 estimators, 0.01 rate and linear type    0.021  20523.725     0.841  \n",
       "300 estimators, 0.1 rate and square type     0.017  19015.790     0.863  \n",
       "300 estimators, 0.1 rate and linear type     0.015  18767.256     0.869  \n",
       "300 estimators, 0.2 rate and square type     0.018  18867.059     0.864  \n",
       "300 estimators, 0.2 rate and linear type     0.015  18782.913     0.868  \n",
       "300 estimators, 0.3 rate and square type     0.015  18584.452     0.872  \n",
       "300 estimators, 0.3 rate and linear type     0.015  18550.458     0.871  \n",
       "300 estimators, 1 rate and square type       0.014  18427.854     0.876  \n",
       "300 estimators, 1 rate and linear type       0.016  18195.216     0.875  \n",
       "350 estimators, 0.0001 rate and square type  0.023  21308.481     0.825  \n",
       "350 estimators, 0.0001 rate and linear type  0.024  21103.812     0.829  \n",
       "350 estimators, 0.001 rate and square type   0.022  21128.064     0.832  \n",
       "350 estimators, 0.001 rate and linear type   0.023  21234.783     0.828  \n",
       "350 estimators, 0.01 rate and square type    0.022  20645.395     0.838  \n",
       "350 estimators, 0.01 rate and linear type    0.021  20452.455     0.842  \n",
       "350 estimators, 0.1 rate and square type     0.018  19037.660     0.861  \n",
       "350 estimators, 0.1 rate and linear type     0.015  18707.895     0.867  \n",
       "350 estimators, 0.2 rate and square type     0.015  18922.455     0.868  \n",
       "350 estimators, 0.2 rate and linear type     0.015  18450.635     0.871  \n",
       "350 estimators, 0.3 rate and square type     0.015  18646.842     0.870  \n",
       "350 estimators, 0.3 rate and linear type     0.015  18607.436     0.870  \n",
       "350 estimators, 1 rate and square type       0.015  18189.847     0.877  \n",
       "350 estimators, 1 rate and linear type       0.015  18188.448     0.874  \n",
       "\n",
       "[98 rows x 6 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can fine tune and see if we are able to find superior model of adaboost\n",
    "#We can adjust number of estimators, the learning rate, and the base estimator used\n",
    "n_estimators = range(50, 400, 50)\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1]\n",
    "lossType = ['square','linear']\n",
    "param = []\n",
    "adapred = []\n",
    "#This is to keep track of which one to use\n",
    "minMAE = 10000000000000\n",
    "optEstAda=0 \n",
    "optRateAda = 0\n",
    "\n",
    "for i in n_estimators:\n",
    "    for j in learning_rate:\n",
    "        for k in lossType:\n",
    "            regr = AdaBoostRegressor(loss=k, learning_rate = j, n_estimators = i)\n",
    "            regr = regr.fit(final_train,y_train)\n",
    "            predFinalAdaBoost = regr.predict(final_test)\n",
    "            adapred.append(predFinalAdaBoost)\n",
    "            param.append(\"{} estimators, {} rate and {} type\".format(i, j, k))\n",
    "            if((mean_absolute_error(y_test, predFinalAdaBoost)<minMAE) and (r2_score(y_test,predFinalAdaBoost)>0.8)):\n",
    "                optEstAda = i\n",
    "                optRateAda = j\n",
    "                opttypeAda = k               \n",
    "getResultTable(param,adapred)\n",
    "#From this, we can figure out which tuning specification we should use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal for AdaBoost was 350 estimators, 1 rate and linear type\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Optimal for AdaBoost was {} estimators, {} rate and {} type\".format(optEstAda, optRateAda, opttypeAda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune parameters for random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels=804 does not match number of samples=482",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-57aa6b71c84a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatureType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mpredFinalRandomForest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mForpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredFinalRandomForest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 327\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 236\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels=804 does not match number of samples=482"
     ]
    }
   ],
   "source": [
    "#We can fine tune and see if we are able to find superior model of randomForest too\n",
    "#We can adjust number of estimators, the learning rate, and the base estimator used\n",
    "n_estimators = range(5, 29, 3)\n",
    "depth = range(1,5,1)\n",
    "samSplit = [2,3]\n",
    "featureType = ['auto','sqrt']\n",
    "paramFor = []\n",
    "Forpred = []\n",
    "#This is to keep track of which one to use\n",
    "minMAE = 10000000000000\n",
    "optEstFor=0 \n",
    "optDepthFor = 0\n",
    "optSplit = 0\n",
    "for i in n_estimators:\n",
    "    for j in depth:\n",
    "        for k in samSplit:\n",
    "            for l in featureType:\n",
    "                regr = RandomForestRegressor(n_estimators = i, max_depth=j, min_samples_split=k ,criterion ='mae', max_features = l)\n",
    "                regr = regr.fit(final_train,y_train)\n",
    "                predFinalRandomForest = regr.predict(final_test)           \n",
    "                Forpred.append(predFinalRandomForest)\n",
    "                paramFor.append(\"{} estimators, {} depth, {} split, and {} type\".format(i, j, k, l))\n",
    "                if((mean_absolute_error(y_test, predFinalRandomForest)<minMAE) and (r2_score(y_test,predFinalRandomForest)>0.8)):\n",
    "                    optEstFor = i\n",
    "                    optDepthFor = j\n",
    "                    optSplit = k\n",
    "                    opttypeFor = l               \n",
    "getResultTable(paramFor,Forpred)\n",
    "#From this, we can figure out which tuning specification we should use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal for Random For was 26 estimators, 4 depth, 3 split and sqrt type\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "\n",
    "print(\"Optimal for Random For was {} estimators, {} depth, {} split and {} type\".format(optEstFor, optDepthFor, optSplit, opttypeFor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally for gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>SE</th>\n",
       "      <th>Jack R2</th>\n",
       "      <th>SE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R-square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Test RMSE, SE, Jack R2, SE, MAE, R-square]\n",
       "Index: []"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can fine tune and see if we are able to find superior model of Gradientboost\n",
    "#We can adjust number of estimators, the learning rate, and the base estimator used\n",
    "n_estimators = range(50, 450, 50)\n",
    "learning_rate = [0.01, 0.2, 0.6, 1]\n",
    "depth = range(1,4,1)\n",
    "samSplit = [2,3]\n",
    "paramGrad = []\n",
    "Gradpred = []\n",
    "#This is to keep track of which one to use\n",
    "minMAE = 10000000000000\n",
    "optEstGrad=0 \n",
    "optRateGrad = 0\n",
    "optDepthGrad = 0\n",
    "optSplitGrad = 0\n",
    "\n",
    "for i in n_estimators:\n",
    "    for j in learning_rate:\n",
    "        for k in depth:\n",
    "            for l in samSplit:\n",
    "                regr = GradientBoostingRegressor(loss='lad', n_estimators = i, learning_rate = j, max_depth = k, min_samples_split=l)\n",
    "                regr = regr.fit(final_train,y_train)\n",
    "                predFinalGradBoostlad = regr.predict(final_test)          \n",
    "                Gradpred.append(predFinalGradBoostlad)\n",
    "                paramFor.append(\"{} estimators, {} rate, {} depth, and {} split\".format(i, j, k, l))\n",
    "                if((mean_absolute_error(y_test, predFinalGradBoostlad)<minMAE) and (r2_score(y_test,predFinalGradBoostlad)>0.8) ):\n",
    "                    optEstGrad = i\n",
    "                    optRateGrad = j\n",
    "                    optDepthGrad = k\n",
    "                    optSplitGrad = l               \n",
    "getResultTable(paramGrad,Gradpred)\n",
    "#From this, we can figure out which tuning specification we should use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal for Gradient boost was 400 estimators, 1 rate,  3 depth, 3 split\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Optimal for Gradient boost was {} estimators, {} rate,  {} depth, {} split\".format(optEstGrad, optRateGrad, optDepthGrad, optSplitGrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_estimators must be greater than zero, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-33860d6a8521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptEstFor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptDepthFor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptSplit\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopttypeFor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mpredFinalRandomForest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbootstrap\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moob_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ChristopherHyland/anaconda/lib/python3.6/site-packages/sklearn/ensemble/base.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             raise ValueError(\"n_estimators must be greater than zero, \"\n\u001b[0;32m--> 109\u001b[0;31m                              \"got {0}.\".format(self.n_estimators))\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_estimators must be greater than zero, got 0."
     ]
    }
   ],
   "source": [
    "#Now using Ensemble methods\n",
    "#http://scikit-learn.org/stable/modules/ensemble.html\n",
    "#http://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "\n",
    "#Bagging and random forest use averages\n",
    "#Boosting combine several weak models into a powerful model\n",
    "#LS first\n",
    "#Random Forest\n",
    "regr = RandomForestRegressor(n_estimators = optEstFor, max_depth=optDepthFor, min_samples_split=optSplit ,criterion ='mae', max_features = opttypeFor)\n",
    "regr = regr.fit(final_train,y_train)\n",
    "predFinalRandomForest = regr.predict(final_test)        \n",
    "\n",
    "#GradientBoosting aka Stochastic gradient boosting\n",
    "regr = GradientBoostingRegressor(n_estimators = optEstGrad, learning_rate = optRateGrad, max_depth = optDepthGrad, min_samples_split=optSplitGrad)\n",
    "regr = regr.fit(final_train,y_train)\n",
    "predFinalGradBoostLS = regr.predict(final_test)\n",
    "\n",
    "#Extremely Random forest\n",
    "regr = ExtraTreesRegressor(max_depth=None, min_samples_split=2)\n",
    "regr = regr.fit(final_train,y_train)\n",
    "predFinalExtRandomForestLS = regr.predict(final_test)\n",
    "\n",
    "regr = BaggingRegressor()\n",
    "regr = regr.fit(final_train,y_train)\n",
    "predFinalBagging = regr.predict(final_test)\n",
    "\n",
    "regr = AdaBoostRegressor(loss=opttypeAda, learning_rate = optRateAda, n_estimators = optEstAda)\n",
    "regr = regr.fit(final_train,y_train)\n",
    "predFinalAdaBoost = regr.predict(final_test)\n",
    "\n",
    "method.append('Random ForestLS')\n",
    "pred.append(predFinalRandomForest)\n",
    "method.append('GradientBoostingLS')\n",
    "pred.append(predFinalGradBoostLS)\n",
    "method.append('ExtraTreesRegressorLS')\n",
    "pred.append(predFinalExtRandomForestLS)\n",
    "method.append('BaggingRegressorLS')\n",
    "pred.append(predFinalBagging)\n",
    "method.append('AdaBoostRegressorLS')\n",
    "pred.append(predFinalAdaBoost)\n",
    "\n",
    "\n",
    "#GradientBoosting\n",
    "regr = GradientBoostingRegressor(loss='lad', n_estimators = optEstGrad, learning_rate = optRateGrad, max_depth = optDepthGrad, min_samples_split=optSplitGrad)\n",
    "regr = regr.fit(final_train,y_train)\n",
    "predFinalGradBoostlad = regr.predict(final_test)   \n",
    "\n",
    "#Extremely Random forest\n",
    "regr = ExtraTreesRegressor(criterion='mae',max_depth=None,min_samples_split=2)\n",
    "regr = regr.fit(final_train,y_train)\n",
    "predFinalExtRandomForestlad = regr.predict(final_test)\n",
    "\n",
    "regr = AdaBoostRegressor(loss='linear')\n",
    "regr = regr.fit(final_train,y_train)\n",
    "predFinalAdaBoostlad = regr.predict(final_test)\n",
    "\n",
    "method.append('GradientBoostingLAD')\n",
    "pred.append(predFinalGradBoostlad)\n",
    "method.append('ExtraTreesRegressorLAD')\n",
    "pred.append(predFinalExtRandomForestLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some new techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Ridge Regression with normalisation of data\n",
    "clf = linear_model.BayesianRidge(normalize=True)\n",
    "clf.fit(final_train,y_train)\n",
    "predFinalBayRidge = clf.predict(final_test)\n",
    "method.append('Bayseian Ridge')\n",
    "pred.append(predFinalBayRidge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN does not work for unsupervised learning/predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I don't have to construct these tables manually \n",
    "def getResultTable(rows, predictions):\n",
    "    columns=['Test RMSE', 'SE', 'Jack R2', 'SE', 'MAE', 'R-square']\n",
    "    results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "    \n",
    "    for row,pred in zip(range(0,len(rows)),predictions):\n",
    "        results.iloc[row,0], results.iloc[row,1] = rmse_jack(y_test, pred)\n",
    "        results.iloc[row,2], results.iloc[row,3] = (r2_jack(y_test, pred))\n",
    "        results.iloc[row,4] = mean_absolute_error(y_test, pred)\n",
    "        results.iloc[row,5] = r2_score(y_test,pred)\n",
    "    return results.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test RMSE</th>\n",
       "      <th>SE</th>\n",
       "      <th>Jack R2</th>\n",
       "      <th>SE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R-square</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LASSO</th>\n",
       "      <td>22302.821</td>\n",
       "      <td>3273.858</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.027</td>\n",
       "      <td>14657.405</td>\n",
       "      <td>0.898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>22198.874</td>\n",
       "      <td>3205.593</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.027</td>\n",
       "      <td>14814.092</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Forward Sel</th>\n",
       "      <td>23308.651</td>\n",
       "      <td>2581.009</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.022</td>\n",
       "      <td>15289.870</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random ForestLS</th>\n",
       "      <td>32890.253</td>\n",
       "      <td>4167.495</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.028</td>\n",
       "      <td>20847.500</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingLS</th>\n",
       "      <td>35093.947</td>\n",
       "      <td>2416.598</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.040</td>\n",
       "      <td>24355.783</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressorLS</th>\n",
       "      <td>22010.162</td>\n",
       "      <td>1882.558</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.011</td>\n",
       "      <td>15477.165</td>\n",
       "      <td>0.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingRegressorLS</th>\n",
       "      <td>23734.254</td>\n",
       "      <td>2445.746</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.015</td>\n",
       "      <td>16902.539</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressorLS</th>\n",
       "      <td>24834.308</td>\n",
       "      <td>2427.106</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.015</td>\n",
       "      <td>18086.837</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingLAD</th>\n",
       "      <td>37042.179</td>\n",
       "      <td>2921.321</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.034</td>\n",
       "      <td>26811.292</td>\n",
       "      <td>0.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressorLAD</th>\n",
       "      <td>22010.162</td>\n",
       "      <td>1882.558</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.011</td>\n",
       "      <td>15477.165</td>\n",
       "      <td>0.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayseian Ridge</th>\n",
       "      <td>23334.675</td>\n",
       "      <td>2593.260</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.025</td>\n",
       "      <td>15543.691</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENET</th>\n",
       "      <td>32421.518</td>\n",
       "      <td>3673.307</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.038</td>\n",
       "      <td>21681.739</td>\n",
       "      <td>0.785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Test RMSE        SE  Jack R2     SE        MAE  \\\n",
       "LASSO                   22302.821  3273.858    0.898  0.027  14657.405   \n",
       "Ridge                   22198.874  3205.593    0.899  0.027  14814.092   \n",
       "Forward Sel             23308.651  2581.009    0.889  0.022  15289.870   \n",
       "Random ForestLS         32890.253  4167.495    0.779  0.028  20847.500   \n",
       "GradientBoostingLS      35093.947  2416.598    0.748  0.040  24355.783   \n",
       "ExtraTreesRegressorLS   22010.162  1882.558    0.901  0.011  15477.165   \n",
       "BaggingRegressorLS      23734.254  2445.746    0.885  0.015  16902.539   \n",
       "AdaBoostRegressorLS     24834.308  2427.106    0.874  0.015  18086.837   \n",
       "GradientBoostingLAD     37042.179  2921.321    0.720  0.034  26811.292   \n",
       "ExtraTreesRegressorLAD  22010.162  1882.558    0.901  0.011  15477.165   \n",
       "Bayseian Ridge          23334.675  2593.260    0.889  0.025  15543.691   \n",
       "ENET                    32421.518  3673.307    0.785  0.038  21681.739   \n",
       "\n",
       "                        R-square  \n",
       "LASSO                      0.898  \n",
       "Ridge                      0.899  \n",
       "Forward Sel                0.889  \n",
       "Random ForestLS            0.779  \n",
       "GradientBoostingLS         0.748  \n",
       "ExtraTreesRegressorLS      0.901  \n",
       "BaggingRegressorLS         0.885  \n",
       "AdaBoostRegressorLS        0.874  \n",
       "GradientBoostingLAD        0.720  \n",
       "ExtraTreesRegressorLAD     0.901  \n",
       "Bayseian Ridge             0.889  \n",
       "ENET                       0.785  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getResultTable(method,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAGGLE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ExtraTreeLAD, Ridge, Lasso, and Bayesian Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using standardised data for ridge and lasso\n",
    "data = pd.read_csv('TrainStandard.csv')\n",
    "y_train = data.pop('SalePrice')\n",
    "\n",
    "y_train\n",
    "mu=y_train.mean()\n",
    "sigma=y_train.std() \n",
    "\n",
    "standardPrice=(y_train-mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle1 = pd.read_csv('TestStandard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RIDGE\n",
    "alphas = np.exp(np.linspace(-10,20,500)) \n",
    "ridge = RidgeCV(alphas=alphas, cv=10)\n",
    "ridge.fit(data, np.ravel(standardPrice))\n",
    "ridge = Ridge(alpha=ridge.alpha_)\n",
    "ridge.fit(data, np.ravel(standardPrice))\n",
    "pred_R_Kaggle = ridge.predict(kaggle1)\n",
    "predTestRidge = (pred_R_Kaggle*sigma) + mu\n",
    "\n",
    "#LASSO\n",
    "lasso = LassoCV(cv=10)\n",
    "lasso.fit(data, np.ravel(standardPrice)) \n",
    "pred_L_Kaggle = lasso.predict(kaggle1)\n",
    "predTestLasso = (pred_L_Kaggle*sigma) + mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using standard dataset now\n",
    "data2 = pd.read_csv('TrainSale1.csv')\n",
    "y_train2 = data2.pop('SalePrice')\n",
    "kaggle2 = pd.read_csv('TestSale1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bayesian Ridge Regression with normalisation of data\n",
    "clf = linear_model.BayesianRidge(normalize=True)\n",
    "clf.fit(data2,y_train2)\n",
    "predTestBayRidge = clf.predict(kaggle2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr = ExtraTreesRegressor(criterion='mae',max_depth=None,min_samples_split=2)\n",
    "regr = regr.fit(data2,y_train2)\n",
    "predTestRandomForest = regr.predict(kaggle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KagglePred1 = (predTestRandomForest+predTestBayRidge+predTestLasso+predTestRidge)/4\n",
    "#This is for the indices\n",
    "ind = np.arange(1,1609)\n",
    "headers = ['Id','Prediction']\n",
    "prediction2 = pd.DataFrame({'Id':ind, 'Prediction':KagglePred1})\n",
    "prediction2\n",
    "#Saving results into CSV file \n",
    "prediction2.to_csv(\"PredictionDay2_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine with forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Forward with non-standardised data\n",
    "fwd = forward()\n",
    "fwd.fit(data2, y_train2)\n",
    "predFinalForward = fwd.predict(kaggle2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "kagglepred3 = (predTestLasso+predTestRidge+predFinalForward+predTestRandomForest)/4\n",
    "#This is for the indices\n",
    "ind = np.arange(1,1609)\n",
    "headers = ['Id','Prediction']\n",
    "prediction3 = pd.DataFrame({'Id':ind, 'Prediction':kagglepred3})\n",
    "prediction3\n",
    "#Saving results into CSV file \n",
    "prediction3.to_csv(\"PredictionDay2_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different model completely and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Train6.csv')\n",
    "y_price = data.pop('SalePrice')\n",
    "\n",
    "alphas = np.exp(np.linspace(-10,20,500)) \n",
    "ridge = RidgeCV(alphas=alphas, cv=10)\n",
    "ridge.fit(data, np.ravel(y_price))\n",
    "ridge = Ridge(alpha=ridge.alpha_)\n",
    "ridge.fit(data, np.ravel(y_price))\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "GBoost.fit(data,y_price)\n",
    "\n",
    "#Forward selection\n",
    "from QBUS2820 import forward\n",
    "\n",
    "fwd = forward()\n",
    "fwd.fit(data, y_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr = AdaBoostRegressor(loss=opttypeAda, learning_rate = optRateAda, n_estimators = optEstAda)\n",
    "regr = regr.fit(data,y_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regr_T = ExtraTreesRegressor(criterion='mae',max_depth=None,min_samples_split=2)\n",
    "regr_T = regr.fit(data,y_price)\n",
    "\n",
    "regr_B = BaggingRegressor()\n",
    "regr_B = regr.fit(data,y_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle = pd.read_csv('Test6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_B = regr_B.predict(kaggle)\n",
    "pred_T = regr_T.predict(kaggle)\n",
    "pred_F = fwd.predict(kaggle)\n",
    "pred_G = GBoost.predict(kaggle)\n",
    "pred_R = ridge.predict(kaggle)\n",
    "pred_A = regr.predict(kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kagglepred4 = (pred_B+pred_T+pred_F+pred_G+pred_R+pred_A)/6\n",
    "#This is for the indices\n",
    "ind = np.arange(1,1609)\n",
    "headers = ['Id','Prediction']\n",
    "prediction4 = pd.DataFrame({'Id':ind, 'Prediction':kagglepred4})\n",
    "prediction4\n",
    "#Saving results into CSV file \n",
    "prediction4.to_csv(\"PredictionDay2_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final attempt on previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "kagglepred5 = (pred_A+pred_R+pred_F+pred_G)/4\n",
    "#This is for the indices\n",
    "ind = np.arange(1,1609)\n",
    "headers = ['Id','Prediction']\n",
    "prediction5 = pd.DataFrame({'Id':ind, 'Prediction':kagglepred5})\n",
    "prediction5\n",
    "#Saving results into CSV file \n",
    "prediction5.to_csv(\"PredictionDay2_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
